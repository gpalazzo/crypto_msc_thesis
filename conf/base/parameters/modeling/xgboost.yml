# if `logreg_optixgboost_optimize_paramsmize_params` is true, then the parameters are gonna be optimized in runtime
xgboost_optimize_params: false

# apply these parameters regardless of optimization or not
xgboost_default_params:
  n_jobs: -1
  use_label_encoder: false
  seed: 0

# if `xgboost_optimize_params` is false, then use the parameters defined below
# if not using `subsample` and `colsample_bytree` parameters the model is deterministic, but increases the chance of overfitting
xgboost_model_params:
  eval_metric: "error"
  n_estimators: 1500
  max_depth: 2
  reg_lambda: 0.26366508987303583
  gamma: 0.12742749857031335
  min_child_weight: 6.951927961775605
  learning_rate: 0.3359818286283781
  objective: "binary:hinge"
  sampling_method: "gradient_based"
  tree_method: "approx"
  booster: "gbtree"
