{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd428de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array #useful to parse values\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_COL = \"window_nbr\"\n",
    "LABEL_COL = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5c71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt_train = catalog.load(\"master_table_train_multic\").set_index(INDEX_COL)\n",
    "mt_test = catalog.load(\"master_table_test_multic\").set_index(INDEX_COL)\n",
    "\n",
    "y_train = mt_train[[LABEL_COL]]\n",
    "y_test = mt_test[[LABEL_COL]]\n",
    "\n",
    "y_pred = catalog.load(\"xgboost_model_predict\")\n",
    "df_model_rpt = catalog.load(\"xgboost_model_reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f92f1",
   "metadata": {},
   "source": [
    "### Base notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4509379",
   "metadata": {},
   "source": [
    "### Evaluate model's split between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9191b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_split = y_train.rename(columns={\"label\": \"train_data\"})\n",
    "y_test_split = y_test.rename(columns={\"label\": \"test_data\"})\n",
    "\n",
    "y_all = y_train_split.merge(y_test_split, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "# evaluate if there's no data leakage between train and test sets\n",
    "y_all.plot(figsize=(15,5), title=\"Timeseries sensical eval\", style=\".\", colormap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269acaf9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Evaluate model's probability\n",
    "- for correct labels, the more delocated to the right the better (meaning it's predicting the right label with high probability)\n",
    "- for incorrect labels, the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = eval(df_model_rpt[\"test_probas\"].values[0])\n",
    "\n",
    "probas_df = pd.DataFrame.from_dict(data=probas, orient=\"index\")\n",
    "probas_df = probas_df.sort_index()\n",
    "\n",
    "df = probas_df.merge(y_test, left_index=True, right_index=True, how=\"inner\")\n",
    "assert df.shape[0] == probas_df.shape[0] == y_test.shape[0]\n",
    "df2 = df.merge(y_pred, left_index=True, right_index=True, how=\"inner\")\n",
    "assert df.shape[0] == df2.shape[0]\n",
    "\n",
    "df_right = df2[df2[\"label\"] == df2[\"y_pred\"]]\n",
    "df_wrong = df2.drop(df_right.index)\n",
    "\n",
    "df_right = df_right.drop(columns=[\"label\"])\n",
    "right_probas = df_right[[\"proba_label_0\", \"proba_label_1\"]].max(axis=1)\n",
    "\n",
    "df_wrong = df_wrong.drop(columns=[\"label\"])\n",
    "wrong_probas = df_wrong[[\"proba_label_0\", \"proba_label_1\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990db8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_probas.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_probas.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc4c0",
   "metadata": {},
   "source": [
    "### Evaluate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_imp = eval(df_model_rpt[\"fte_importance\"][0])\n",
    "\n",
    "data = {\"features\": fte_imp.keys(),\n",
    "       \"importance\": fte_imp.values()}\n",
    "\n",
    "df_fte_imp = pd.DataFrame.from_dict(data=data)\n",
    "df_fte_imp = df_fte_imp.sort_values(by=\"importance\", ascending=True)\n",
    "\n",
    "df_fte_imp.set_index(\"features\").plot(kind=\"barh\", figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19e234",
   "metadata": {},
   "source": [
    "### Evaluate: target class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba610d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance = eval(df_model_rpt[\"label_class_balance\"][0])\n",
    "\n",
    "data = {\"class\": class_balance.keys(),\n",
    "       \"percentage\": class_balance.values()}\n",
    "\n",
    "df_cls_blc = pd.DataFrame.from_dict(data=data)\n",
    "\n",
    "df_cls_blc.set_index(\"class\").plot(kind=\"bar\", figsize=(5, 3), ylabel=\"percentage\", legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490960b",
   "metadata": {},
   "source": [
    "### Evaluation: plot predicted and actual labels\n",
    "- if model's accuracy is 100%, then this plot would have only one color and blank spaces\n",
    "- times were there's a second color, it means there's a model's wrong prediction\n",
    "- this chart helps understanding if the wrong predictions are concentrated in a particular time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the points where there's wrong predictions through time\n",
    "# the biggest the overlap, the better\n",
    "y_test_pred = y_test.rename(columns={\"label\": \"y_true\"}) \\\n",
    "                .merge(y_pred, left_index=True, right_index=True, how=\"inner\")\n",
    "assert y_test_pred.shape[0] == y_test.shape[0] == y_pred.shape[0]\n",
    "\n",
    "y_test_pred.plot(figsize=(15, 5), colormap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f13a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (crypto_thesis)",
   "language": "python",
   "name": "kedro_crypto_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
