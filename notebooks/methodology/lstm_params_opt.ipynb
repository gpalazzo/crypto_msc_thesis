{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876f314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81902b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crypto_thesis.data_domains.modeling.lstm import _build_lstm_timestamps_seq, lstm_model_predict\n",
    "from pprint import pprint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.engine.sequential import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440db2ef",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = [\"label\"]\n",
    "INDEX_COL = \"window_nbr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_train_multic = catalog.load(\"master_table_train_multic\")\n",
    "mt_test_multic = catalog.load(\"master_table_test_multic\")\n",
    "seq_length = catalog.load(\"params:lstm_timestamp_seq_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddd60c",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed\n",
    "np.random.seed(0)\n",
    "# set tensorflow seed\n",
    "tf.random.set_seed(0)\n",
    "SHUFFLE = False\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894191",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table_train = mt_train_multic.set_index(INDEX_COL)\n",
    "X_train, y_train = master_table_train.drop(columns=TARGET_COL), master_table_train[TARGET_COL]\n",
    "\n",
    "X_train_scaled_seq, y_train_scaled_seq = _build_lstm_timestamps_seq(X=X_train,\n",
    "                                                                    y=y_train,\n",
    "                                                                    seq_length=seq_length)\n",
    "\n",
    "master_table_test = mt_test_multic.set_index(INDEX_COL)\n",
    "X_test, y_test = master_table_test.drop(columns=TARGET_COL), master_table_test[TARGET_COL]\n",
    "\n",
    "X_test_scaled_seq, y_test_scaled_seq = _build_lstm_timestamps_seq(X=X_test,\n",
    "                                                                    y=y_test,\n",
    "                                                                    seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm(optimizer: str) -> Sequential:\n",
    "\n",
    "    # parameters\n",
    "    LAYERS = [20, 20, 20, 1] #[10, 10, 10, 1]                # number of units in hidden and output layers\n",
    "    N = X_train_scaled_seq.shape[2]                 # number of features\n",
    "    LAMBD = 0.005 #0.001                         # lambda in L2 regularizaion\n",
    "    DP = 0.0 #0.0                             # dropout rate\n",
    "    RDP = 0.0 #0.0                            # recurrent dropout rate\n",
    "\n",
    "    # model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        input_shape=(seq_length, N),\n",
    "        units=LAYERS[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=True,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(\n",
    "        units=LAYERS[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=True,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(\n",
    "        units=LAYERS[2],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(\n",
    "        units=LAYERS[3],\n",
    "        activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_params = {\n",
    "    'batch_size': [4, 16],\n",
    "    'optimizer': ['SGD', 'RMSprop', 'Adam']\n",
    "    }\n",
    "model = KerasClassifier(build_fn=create_model_lstm, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=lstm_model_params, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled_seq, y_train_scaled_seq)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "print(\"*\" * 100)\n",
    "print()\n",
    "print(f\"Best accuracy of {grid_result.best_score_}\") \n",
    "print()\n",
    "print(\"Best parameters:\")\n",
    "pprint(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0680ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_lstm(optimizer=best_params[\"optimizer\"])\n",
    "\n",
    "M_TRAIN = X_train_scaled_seq.shape[0]           # number of training examples (2D)\n",
    "M_TEST = X_test_scaled_seq.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "BATCH = best_params[\"batch_size\"]                          # batch size\n",
    "\n",
    "lr_decay = ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            patience=1,\n",
    "            verbose=0,\n",
    "            factor=0.5,\n",
    "            min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                        patience=EPOCHS, verbose=1, mode='auto',\n",
    "                        baseline=0, restore_best_weights=True)\n",
    "\n",
    "train_history = model.fit(X_train_scaled_seq, y_train_scaled_seq,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_test_scaled_seq[:M_TEST], y_test_scaled_seq[:M_TEST]),\n",
    "                    shuffle=SHUFFLE, verbose=0,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "y_pred = lstm_model_predict(model=model, master_table_test=mt_test_multic, seq_length=seq_length)\n",
    "print(y_pred[\"y_pred\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c065c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (crypto_thesis)",
   "language": "python",
   "name": "kedro_crypto_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
