{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876f314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81902b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crypto_thesis.data_domains.modeling.lstm import _build_lstm_timestamps_seq, lstm_model_predict\n",
    "from pprint import pprint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.engine.sequential import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440db2ef",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = [\"label\"]\n",
    "INDEX_COL = \"window_nbr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_train_multic = catalog.load(\"master_table_train_multic\")\n",
    "mt_test_multic = catalog.load(\"master_table_test_multic\")\n",
    "seq_length = catalog.load(\"params:lstm_timestamp_seq_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddd60c",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed\n",
    "np.random.seed(0)\n",
    "# set tensorflow seed\n",
    "tf.random.set_seed(0)\n",
    "SHUFFLE = False\n",
    "\n",
    "EPOCHS = 50\n",
    "N_SPLITS = 5\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894191",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table_train = mt_train_multic.set_index(INDEX_COL).sort_index().reset_index(drop=True)\n",
    "X_train, y_train = master_table_train.drop(columns=TARGET_COL), master_table_train[TARGET_COL]\n",
    "\n",
    "X_train_scaled_seq, y_train_scaled_seq = _build_lstm_timestamps_seq(X=X_train,\n",
    "                                                                    y=y_train,\n",
    "                                                                    seq_length=seq_length)\n",
    "\n",
    "master_table_test = mt_test_multic.set_index(INDEX_COL).sort_index().reset_index(drop=True)\n",
    "X_test, y_test = master_table_test.drop(columns=TARGET_COL), master_table_test[TARGET_COL]\n",
    "\n",
    "X_test_scaled_seq, y_test_scaled_seq = _build_lstm_timestamps_seq(X=X_test,\n",
    "                                                                    y=y_test,\n",
    "                                                                    seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm(optimizer: str) -> Sequential:\n",
    "\n",
    "    # parameters\n",
    "    LAYERS = [20, 20, 20, 1]\n",
    "    N = X_train_scaled_seq.shape[2]\n",
    "    LAMBD = 0.005\n",
    "    DP = 0.0\n",
    "    RDP = 0.0\n",
    "\n",
    "    # model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_shape=(seq_length, N),\n",
    "        units=LAYERS[0],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=True,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units=LAYERS[1],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=True,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units=LAYERS[2],\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        kernel_regularizer=l2(LAMBD),\n",
    "        recurrent_regularizer=l2(LAMBD),\n",
    "        dropout=DP,\n",
    "        recurrent_dropout=RDP,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        stateful=False,\n",
    "        unroll=False\n",
    "                ))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(\n",
    "        units=LAYERS[3],\n",
    "        activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "lstm_model_params = {\n",
    "    'batch_size': [4, 16],\n",
    "    'optimizer': ['SGD', 'RMSprop', 'Adam']\n",
    "    }\n",
    "model = KerasClassifier(build_fn=create_model_lstm, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "cv_results = {}\n",
    "tss = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "for i, (train_index, test_index) in enumerate(tss.split(X_train), 1):\n",
    "    X_train_split, X_test_split = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_split, y_test_split = y_train.iloc[train_index, :], y_train.iloc[test_index, :]\n",
    "\n",
    "    X_train_scaled_seq, y_train_scaled_seq = _build_lstm_timestamps_seq(X=X_train_split,\n",
    "                                                                    y=y_train_split,\n",
    "                                                                    seq_length=seq_length)\n",
    "    \n",
    "    X_test_scaled_seq, y_test_scaled_seq = _build_lstm_timestamps_seq(X=X_test_split,\n",
    "                                                                    y=y_test_split,\n",
    "                                                                    seq_length=seq_length)\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, \n",
    "                        param_grid=lstm_model_params, \n",
    "                        n_jobs=-1,\n",
    "                        scoring=\"accuracy\")\n",
    "    grid_result = grid.fit(X_train_scaled_seq, y_train_scaled_seq)\n",
    "\n",
    "    cv_results[f\"fold_{i}\"] = {\"score\": grid_result.best_score_,\n",
    "                               \"params\": grid_result.best_params_}\n",
    "    \n",
    "end = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -999\n",
    "best_fold = \"\"\n",
    "scores = []\n",
    "\n",
    "for fold, params in cv_results.items():\n",
    "    if params[\"score\"] > best_score:\n",
    "        best_score = params[\"score\"]\n",
    "        best_fold = fold\n",
    "    scores.append(params[\"score\"])\n",
    "\n",
    "best_params = cv_results[best_fold][\"params\"]\n",
    "\n",
    "print(\"Reporting:\")\n",
    "print(f\"Best score: {max(scores)}\")\n",
    "print(f\"Min: {min(scores)}, avg: {round(np.average(scores), 4)} and std: {round(np.std(scores, ddof=1), 4)} of scores\")\n",
    "print(f\"Time elapsed (seconds): {end-start}\")\n",
    "print()\n",
    "print(\"Best parameters:\")\n",
    "pprint(best_params)\n",
    "print()\n",
    "print(\"CV results:\")\n",
    "pprint(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0680ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model_lstm(optimizer=best_params[\"optimizer\"])\n",
    "model = create_model_lstm(optimizer=\"SGD\")\n",
    "\n",
    "M_TRAIN = X_train_scaled_seq.shape[0]\n",
    "M_TEST = X_test_scaled_seq.shape[0]\n",
    "# BATCH = best_params[\"batch_size\"]\n",
    "BATCH = 4\n",
    "\n",
    "lr_decay = ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            patience=1,\n",
    "            verbose=0,\n",
    "            factor=0.5,\n",
    "            min_lr=1e-8)\n",
    "\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                        patience=EPOCHS, verbose=1, mode='auto',\n",
    "                        baseline=0, restore_best_weights=True)\n",
    "\n",
    "train_history = model.fit(X_train_scaled_seq, y_train_scaled_seq,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_test_scaled_seq[:M_TEST], y_test_scaled_seq[:M_TEST]),\n",
    "                    shuffle=SHUFFLE, verbose=0,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "\n",
    "y_pred = lstm_model_predict(model=model, master_table_test=mt_test_multic, seq_length=seq_length)\n",
    "print(y_pred[\"y_pred\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c065c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (crypto_thesis)",
   "language": "python",
   "name": "kedro_crypto_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
